<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kun Wang</title>

    <meta name="author" content="Kun Wang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/icon/person.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kun Wang (王琨)
                </p>
                <p class="justify-text">
                    I am currently a final-year Ph.D. candidate at the <a href="http://www.patternrecognition.asia/">PCALab</a> group, Nanjing University of Science and Technology, Nanjing, China.
                    I am privileged to work under the supervision of Prof. <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang</a> and co-supervision of Prof. <a
                      href="https://sites.google.com/view/junlineu/">Jun Li</a>.
                    I warmly welcome opportunities for discussion and collaboration—please feel free to reach out &#128516;.
                </p>
                <p style="text-align:center">
                  <a href="mailto:kunwang@njust.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/w2kun">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:38%;max-width:38%">
                <img style="width:68%;max-width:68%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/kunwang.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
              <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests encompass computer vision and machine learning, with a focus on 3D-related tasks including depth estimation, depth completion, and reconstruction, <i>etc</i>.
                </p>
              </td>
            </tr>
            </tbody>
          </table>

<!--    script to open the bibtex in a new page-->
    <script>
        function openBibtexInNewPage(filePath, title) {
            // Open a new window or tab
            const newWindow = window.open('', '_blank');

            fetch(filePath)
                .then(response => {
                    if (!response.ok) {
                        throw new Error('File not found');
                    }
                    return response.text();
                })
                .then(data => {
                    // Add content to the new window
                    newWindow.document.write('<html><head><title>' + title + '</title></head><body>');
                    newWindow.document.write('<link rel="shortcut icon" href="images/icon/citation.png" type="image/x-icon">')
                    // newWindow.document.write('<h1>BibTeX</h1>');
                    newWindow.document.write('<pre>' + data + '</pre>'); // Use <pre> to preserve formatting
                    newWindow.document.write('</body></html>');
                    newWindow.document.close();
                })
                .catch(error => {
                    newWindow.document.write('<p>Error loading file: ' + error.message + '</p>');
                });
        }
    </script>

  <table class="papertable" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

    <!--    list begins-->

    <tr class="table_row">
      <td class="leftcol">
        <div class="image-container">
            <img class="image image1" src="images/lpnet/fig1.png">
            <img class="image image2" src='images/lpnet/fig2.png'>
        </div>
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://arxiv.org/abs/2502.07289">Learning Inverse Laplacian Pyramid for Progressive Depth Completion</a>
          </span>
        <br>
        <strong>Kun Wang</strong>,
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <a href="https://fanjunkai1.github.io/">Junkai Fan</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang &#9993</a>
        <br>
        <em>arXiv</em>, 2025 /
            <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/lpnet.bib', 'LP-Net')">BibTeX</a>

        <p class="justify-text">
        LP-Net introduces an innovative multi-scale, progressive depth completion approach based on Laplacian Pyramid decomposition.
            It begins by establishing a global context of the scene structure, then progressively refines local details through a novel selective filtering mechanism at each scale. LP-Net achieves SOTA performance on both indoor and outdoor datasets, while maintains
            high computational efficiency. At the time of submission, it ranks 1st at KITTI online leaderboard among peer-reviewed methods.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src="images/signet/SigNet.png">
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://arxiv.org/pdf/2412.19225">Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion</a>
          </span>
        <br>
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang</a>,
		<strong>Kun Wang</strong>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang &#9993</a>
        <br>
        <em>arXiv</em>, 2024 /
            <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/signet.bib', 'SigNet')">BibTeX</a>

        <p class="justify-text">
       We propose a novel degradation-aware framework SigNet that transforms depth completion into depth enhancement for the first time.
			  SigNet eliminates the mismatch and ambiguity caused by direct convolution over irregularly sampled sparse data.
			  Meanwhile, it builds a self-supervised degradation bridge between coarse depth and targeted dense depth for effective RGB-D fusion.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src="images/dcl/demo.gif">
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://arxiv.org/pdf/2412.11395">DCL: Depth-Centric Dehazing and Depth-Estimation from Real-World Hazy Driving Video</a>
          </span>
        <br>
        <a href="https://fanjunkai1.github.io/">Junkai Fan</a>,
        <strong>Kun Wang</strong>,
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <a href="https://cschenxiang.github.io/">Xiang Chen</a>,
        <a href="">Shangbin Gao</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang &#9993</a>
        <br>
        <em>AAAI</em>, 2025 / <a href="https://fanjunkai1.github.io/projectpage/DCL/index.html">Project Page</a>  /
				  <a href="https://github.com/fanjunkai1/DCL">Github</a> /
				  <a href="https://www.youtube.com/watch?v=8FYw-MHksq4">Video</a> /
            <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/dcl.bib', 'DCL')">BibTeX</a>

        <p class="justify-text">
        We propose a novel depth-centric learning framework that combines the atmospheric scattering model(ASM) model with the brightness consistency constraint (BCC) constraint.
				  The core idea is to use a shared depth estimation network for both ASM and BCC.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <div class="image-container">
            <img class="image image1" src="images/dcdepth/fig1.png">
            <img class="image image2" src='images/dcdepth/framework.jpg'>
        </div>
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://arxiv.org/abs/2410.14980">DCDepth: Progressive Monocular Depth Estimation in Discrete Cosine Domain</a>
          </span>
        <br>
        <strong>Kun Wang</strong>,
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <a href="https://fanjunkai1.github.io/">Junkai Fan</a>,
        Wanlu Zhu,
        <a href="https://implus.github.io/">Xiang Li</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang &#9993</a>
        <br>
        <em>NeurIPS</em>, 2024 / <a href="https://github.com/w2kun/DCDepth">Github</a> /
          <a href="https://neurips.cc/media/neurips-2024/Slides/96698.pdf">Slides</a> /
          <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202024/96698.png?t=1731992895.0685937">Poster</a> /
            <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/dcdepth.bib', 'DCDepth')">BibTeX</a>

        <p class="justify-text">
        DCDepth redefines monocular depth estimation by moving it into the discrete cosine domain, where it models local correlations within depth patches. By exploiting frequency characteristics,
            DCDepth progressively estimates depth, starting with low-frequency components and then refining with high-frequency details,
            facilitating a global-to-local depth estimation approach.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src="images/dvd/pipeline.png">
          <img class="figure" src="images/dvd/demo.gif">
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://arxiv.org/pdf/2405.09996">Driving-Video Dehazing with Non-Aligned Regularization for Safety Assistance</a>
          </span>
        <br>
        <a href="https://fanjunkai1.github.io/">Junkai Fan</a>,
          <a href="https://wengjiangwei.github.io/">Jiangwei Weng</a>,
          <strong>Kun Wang</strong>,
          <a href="https://yijun-yang.github.io/">Yijun Yang</a>,
          <a href="http://www.patternrecognition.asia/qian/">Jianjun Qian</a>,
          <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
          <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=">Jian Yang &#9993</a>
        <br>
        <em>CVPR</em>, 2024 / <a href="https://fanjunkai1.github.io/projectpage/DVD/index.html">Project Page</a>  /
				  <a href="https://github.com/fanjunkai1/DVD">Github</a>  /
				  <a href="https://youtu.be/BHFVx8yv4SY">Video</a> /
                    <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/dvd.bib', 'DVD')">BibTeX</a>

        <p class="justify-text">
        We present an innovative video dehazing framework for real-world driving scenarios, addressing temporal and spatial misalignment
				  challenges with non-aligned hazy/clear video pairs and a reference frame matching module.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src="images/tpvd/TPVD.jpg">
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://arxiv.org/pdf/2403.15008.pdf">Tri-Perspective View Decomposition for Geometry-Aware Depth Completion</a>
          </span>
        <br>
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <a href="https://scholar.google.com/citations?user=I_WdxZwAAAAJ&hl=zh-CN&oi=sra">Yuankai Lin</a>,
        <strong>Kun Wang</strong>,
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=anGhGdYAAAAJ">Yupeng Zheng</a>,
        <a href="https://scholar.google.com/citations?user=kDaztnkAAAAJ&hl=en">Yufei Wang</a>,
        <a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
        <br>
        <em>CVPR</em>, 2024, <font color="red"><strong>oral</strong></font> /
          <a href="projectpage/TOFDC/index.html">Project Page</a> /
          <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/tpvd.bib', 'TPVD')">BibTeX</a>

        <p class="justify-text">
        TPVD decomposes 3D point cloud into three views to capture the fine-grained 3D geometry of scenes. TPV Fusion and GSPN modules are proposed to refine the depth.
			  Furthermore, we build a novel depth completion dataset named TOFDC, acquired by a smartphone equipped with a time-of-flight (TOF) sensor.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
        <div class="image-container">
            <img class="image image1" src="images/altnerf/figure1.jpg">
            <img class="image image2" src='images/altnerf/cat.gif'>
        </div>
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28360">AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization </a>
          </span>
        <br>
        <strong>Kun Wang</strong>,
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        Huang Tian,
        <a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>,
        <a href="https://implus.github.io/">Xiang Li</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
        <br>
        <em>AAAI</em>, 2024 /
        <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/altnerf.bib', 'AltNeRF')">BibTeX</a>

        <p class="justify-text">
           NeRF optimization frequently encounters local minima due to inaccurate camera poses and absent 3D supervision. AltNeRF employs an alternating optimization approach, starting with coarse depth and pose from a self-supervised estimator,
            to simultaneously refine camera pose, scene depth, and learn the NeRF representation, thereby substantially lowering the cost of high-quality NeRF creation.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src="images/dul/DUL.png">&nbsp
          <img class="figure" src="images/dul/DUL-vis.jpg"/>
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://openreview.net/pdf?id=0tLjOxqjLS">Distortion and Uncertainty Aware Loss for Panoramic Depth Completion</a>
          </span>
        <br>
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <a href="https://implus.github.io/">Xiang Li</a>,
        <strong>Kun Wang</strong>,
        <a href="https://shuochenya.github.io/">Shuo Chen &#9993</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a>
        <br>
        <em>ICML</em>, 2023 / <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/dul.bib', 'DUL')">BibTeX</a>

        <p class="justify-text">
           Standard MSE or MAE loss function is commonly used in limited field-of-vision depth completion, treating each pixel equally under a basic assumption that all pixels have same contribution during optimization.
			  However, the assumption is inapplicable to panoramic data due to its latitude-wise distortion and high uncertainty nearby textures and edges.
			  To handle these challenges, this paper proposes the distortion and uncertainty aware loss (DUL) that consists of a distortion-aware loss and an uncertainty-aware loss.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src="images/desnet/DesNet-vis.jpg">
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25415/25187">DesNet: Decomposed Scale-Consistent Network for Unsupervised Depth Completion</a>
          </span>
        <br>
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <strong>Kun Wang</strong>,
        <a href="https://implus.github.io/">Xiang Li</a>,
        <a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
        <br>
        <em>AAAI</em>, 2023, <font color="red"><strong>oral</strong></font> / <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/desnet.bib', 'DesNet')">BibTeX</a>

        <p class="justify-text">
           DesNet first introduces a decomposed scale-consistent learning strategy, which disintegrates the absolute depth into relative depth prediction and global scale estimation, contributing to individual learning benefits.
		  Extensive experiments show the superiority of DesNet on KITTI benchmark, ranking 1st and surpassing the second best more than 12% in RMSE.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src="images/rignet/RigNet.jpg">
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://arxiv.org/pdf/2107.13802.pdf">RigNet: Repetitive Image Guided Network for Depth Completion</a>
          </span>
        <br>
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <strong>Kun Wang</strong>,
        <a href="https://implus.github.io/">Xiang Li</a>,
        <a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
        <br>
        <em>ECCV</em>, 2022 / <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/rignet.bib', 'RigNet')">BibTeX</a>

        <p class="justify-text">
          RigNet explores a repetitive design for depth completion to tackle the blurry guidance in image and unclear structure in depth.
				  Extensive experiments show that RigNet achieves superior or competitive results on KITTI benchmark and NYUv2 dataset.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src="images/m3pt/MMMPT.jpg">&nbsp
          <img class="figure" src="images/m3pt/MMMPT-vis.jpg">
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://arxiv.org/pdf/2203.09855.pdf">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion</a>
          </span>
        <br>
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <a href="https://implus.github.io/">Xiang Li*</a>,
        <strong>Kun Wang</strong>,
        <a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
        <br>
        <em>ECCV</em>, 2022 / <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/m3pt.bib', 'MMMPT')">BibTeX</a>

        <p class="justify-text">
          For the first time, we enable the masked pre-training in a Convolution-based multi-modal task, instead of the Transformer-based single-modal task.
		  What's more, we introduce the panoramic depth completion, a new task that facilitates 3D reconstruction.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src='images/idsr/IDSR.jpg'>
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://drive.google.com/file/d/1qMOQz2sfci_ifRO2aSW5KyyDFKwWvLSW/view">Learning Complementary Correlations for Depth Super-Resolution with Incomplete Data in Real World</a>
          </span>
        <br>
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <strong>Kun Wang</strong>,
        <a href="https://implus.github.io/">Xiang Li</a>,
        <a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>,
        <a href="https://scholar.google.com/citations?user=qovg0wcAAAAJ&hl=zh-CN&oi=ao">Guangyu Li &#9993</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a>
        <br>
        <em>TNNLS</em>, 2022 / <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/idsr.bib', 'IDSR')">BibTeX</a>

        <p class="justify-text">
          Motivated by pratical applications, this paper introduces a new task, i.e., incomplete depth super-resolution (IDSR),
				  which recovers dense and high-resolution depth from incomplete and low-resolution one.
        </p>
      </td>
    </tr>

    <tr class="table_row">
      <td class="leftcol">
          <img class="figure" src='images/rnw/fig1.png'>
      </td>
      <td class="rightcol">
          <span class="papertitle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Regularizing_Nighttime_Weirdness_Efficient_Self-Supervised_Monocular_Depth_Estimation_in_the_ICCV_2021_paper.html">
                  Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark
              </a>
          </span>
        <br>
        <strong>Kun Wang*</strong>,
        <a href="https://jessezhang92.github.io/">Zhenyu Zhang*</a>,
        <a href="https://yanzq95.github.io/">Zhiqiang Yan</a>,
        <a href="https://implus.github.io/">Xiang Li</a>,
        <a href="https://scholar.google.com/citations?user=CCnckVAAAAAJ&hl=zh-CN">Baobei Xu</a>,
        <a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
        <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
        <br>
        <em>ICCV</em>, 2021 /
        <a href="https://github.com/w2kun/RNW">Github</a> /
          <a href="javascript:void(0);" onclick="openBibtexInNewPage('bibtex/rnw.bib', 'RNW')">BibTeX</a>

        <p class="justify-text">
        RNW first introduces a self-supervised monocular depth estimation framework that directly trained on the real-world nighttime images. It leverages distribution knowledge from unpaired depth maps to prevent incorrect model training,
            and further improves the prediction accuracy with brightness-consistent image enhancement and statistics-based pixel removel.
        </p>
      </td>
    </tr>


  </tbody>
  </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Academic Service</h2>
            <ul>
                <li><strong>Conference reviewer:</strong>
                    <ul>
                        <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
                        <li>IEEE International Conference on Computer Vision (ICCV)</li>
                        <li>European Conference on Computer Vision (ECCV)</li>
                        <li>Conference on Neural Information Processing Systems (NeurIPS)</li>
                        <li>International Conference on Learning Representations (ICLR)</li>
                        <li>International Conference on Machine Learning (ICML)</li>
                        <li>AAAI Conference on Artificial Intelligence (AAAI)</li>
                    </ul>
                </li>
            </ul>
            <ul>
              <li><strong>Journal reviewer:</strong>
                <ul>
                    <li>Image and Vision Computing</li>
                </ul>
              </li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:center;font-size:small;">
              The website template is modified from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's</a>. Thanks to him!
            </p>
          </td>
        </tr>
      </tbody>
    </table>

  </body>
</html>
